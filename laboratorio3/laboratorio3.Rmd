---
title: "Laboratorio Nº3"
subtitle: Análisis de Datos
author: "Mauricio Vargas"
highlighter: highlight.js
job: Universidad Nacional Andrés Bello
logo        : logounab.png
hitheme: tomorrow
framework: io2012
url:
  assets: ../assets
  lib: ../libraries
widgets: mathjax
mode        : selfcontained # {standalone, draft}
<!-- knit : slidify::knit2slides --> 
knit : slidify::knit2slides
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
# make this an external chunk that can be included in any file
library(knitr) 
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Modelo de Regresión Básico
* Mínimos cuadrados es una herramienta de estimación.
* Para realizar inferencia se desarrolla un modelo probabilístico de regresión lineal
$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_{i}
$$
* Aquí $\varepsilon_{i}$ se asume iid $N(0, \sigma^2)$. 
* Note que $E[Y_i ~|~ X_i = x_i] = \mu_i = \beta_0 + \beta_1 x_i$
* Note que $Var(Y_i ~|~ X_i = x_i) = \sigma^2$.
* La estimación por ML de $\beta_0$ y $\beta_1$ coincide con la estimación por OLS
  $$\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X$$
* $E[Y ~|~ X = x] = \beta_0 + \beta_1 x$
* $Var(Y ~|~ X = x) = \sigma^2$

---

## Interpretación de los coeficientes (1)

### Intercepto
* $\beta_0$ es el valor esperado del output cuando el input es 0
$$
E[Y | X = 0] =  \beta_0 + \beta_1 \times 0 = \beta_0
$$
* Note que esto no siempre es de interés, por ejemplo cuando $X=0$ es imposible o está fuera del rango de los datos (e.g. Si $X$ corresponde a presión sanguínea, estatura, etc.)
* Considere que
$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
= \beta_0 + a \beta_1 + \beta_1 (X_i - a) + \varepsilon_i
= \tilde \beta_0 + \beta_1 (X_i - a) + \varepsilon_i
$$
Entonces, si desplazamos $X$ en $a$ unidades cambia el intercepto pero no la pendiente.  menudo $a$ se fija en $\bar X$ tal que  el intercepto se interpreta como la respuesta esperada en el valor promedio de $X$.

---

## Interpretación de los coeficientes (2)

### Pendiente

* $\beta_1$ es el cambio esperado en el output cuando el input cambia en una unidad
$$
E[Y ~|~ X = x+1] - E[Y ~|~ X = x] =
\beta_0 + \beta_1 (x + 1) - (\beta_0 + \beta_1 x ) = \beta_1
$$
* Considere el impacto de cambiar las unidades (medición) de $X$
$$
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
= \beta_0 + \frac{\beta_1}{a} (X_i a) + \varepsilon_i
= \beta_0 + \tilde \beta_1 (X_i a) + \varepsilon_i
$$
* Entonces, la multiplicación de $X$ por un factor $a$ resulta en que se divide el coeficiente por el mismo factor $a$.
* Si queremos predecir el output dado un valor del input, digamos $X$, el modelo de regresión predice
  $$
  \hat \beta_0 + \hat \beta_1 X
  $$

---

## Interpretación de los coeficientes (3)

### Ejemplo

* Si $X$ es la estatura en $m$ e $Y$ es el peso en $kg$. Entonces $\beta_1$ es $kg/m$. Convirtiendo $X$ en $cm$ implica multiplicar $X$ por $100 cm/m$. Para obtener $\beta_1$ en las unidades correctas, tenemos que dividir por $100 cm /m$ y así se tendrán las unidades correctas. 
$$
X m \times \frac{100cm}{m} = (100 X) cm
~~\mbox{y}~~
\beta_1 \frac{kg}{m} \times\frac{1 m}{100cm} = 
\left(\frac{\beta_1}{100}\right)\frac{kg}{cm}
$$

---

## Ejemplo: Base de datos `diamond` (1)

### Datos

* Precio de los diamantes (en dólares de Singapur)
* Peso de los diamantes (en quilates)
* Quilate = medida estándar del peso de un diamante = 0,2 $g$
* Para obtener los datos hay que usar `library(UsingR); data(diamond)`

---

## Ejemplo: Base de datos `diamond` (2)

### Gráfico
```{r, echo = FALSE, fig.height=5,fig.width=5}
library(UsingR)
data(diamond)
library(ggplot2)
g = ggplot(diamond, aes(x = carat, y = price))
g = g + xlab("Mass (carats)")
g = g + ylab("Price (SIN $)")
g = g + geom_point(size = 7, colour = "black", alpha=0.5)
g = g + geom_point(size = 5, colour = "blue", alpha=0.2)
g = g + geom_smooth(method = "lm", colour = "black")
g
```

---

## Ejemplo: Base de datos `diamond` (3)

### Ajuste del modelo de regresión
```{r}
fit <- lm(price ~ carat, data = diamond)
coef(fit)
```

* Se estima un aumento esperado de `r round(coef(fit)[2], 2)` dólares de Singapur en el precio por un aumento de un quilate en el precio del diamante.
* El intercepto `r round(coef(fit)[1], 2)` corresponde al precio esperado de un diamante de 0 quilates.

Si se quiere información más detallada se usa `summary(fit)`

---

## Ejemplo: Base de datos `diamond` (4)

### Obtención de un intercepto interpretable
Se puede escribir el modelo usando la desviación con respecto a la media ($X-\bar{X}$) como input.
```{r, echo = TRUE}
fit2 <- lm(price ~ I(carat - mean(carat)), data = diamond)
coef(fit2)
```

Entonces $`r round(coef(fit2)[1], 1)` es el precio esperado para un diamante de peso promedio que en el caso de los datos corresponde a `r mean(diamond$carat)` quilates.

---

## Ejemplo: Base de datos `diamond` (5)

### Cambio de escala
* Un incremento de 1 quilate es muy grande, ¿qué se esperaría si el peso aumenta 1/10 quilates?
* Se puede dividir el coeficiente por 10.
* Se espera un aumento de `r round(coef(fit)[2], 2) / 10` dólares de Singapur en el precio por cada  1/10 quilates que aumenta el precio.
* Esto es lo mismo que cambiar la escala de $X$ y ajustar la regresión
```{r, echo = TRUE}
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
```

---

## Ejemplo: Base de datos `diamond` (5)

### Predicción del precio de un diamante

Supongamos que tenemos tres diamantes cuyos pesos son 0.16, 0,27 y 0,34 quilates. La estimación de su precio se obtiene de la siguiente forma:
```{r, echo = TRUE, eval=FALSE}
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(carat = newx))
```
```{r, echo = TRUE}
predict(fit, newdata = data.frame(carat = newx))
```

---

## Ejemplo: Base de datos `diamond` (6)

### Gráfico para interpretar la regresión

<!-- * Valores observados de los $X$ de la base de datos $\rightarrow$ color azul
* Valores esperados de los $X$ de la base de datos $\rightarrow$ color rojo
* Valores estimados de los $X$ nuevos (los 3 diamantes de la parte anterior) $\rightarrow$  líneas rectas -->
```{r, echo = FALSE, fig.height=5,fig.width=5}
data(diamond)
newy <- coef(fit)[1] + newx + coef(fit)[2] * newx
plot(diamond$carat, diamond$price,  
     xlab = "Peso (quilates)", 
     ylab = "Precio (dolares de Singapur)", 
     bg = "royalblue", 
     xlim=c(0.1, 0.4),
     col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(fit, lwd = 2)
points(diamond$carat, predict(fit), pch = 19, col = "red")
lines(c(0.16, 0.16, 0.1), 
      c(200, coef(fit)[1] + coef(fit)[2] * 0.16,
      coef(fit)[1] + coef(fit)[2] * 0.16))
lines(c(0.27, 0.27, 0.1), 
      c(200, coef(fit)[1] + coef(fit)[2] * 0.27,
        coef(fit)[1] + coef(fit)[2] * 0.27))
lines(c(0.34, 0.34, 0.1), 
      c(200, coef(fit)[1] + coef(fit)[2] * 0.34,
        coef(fit)[1] + coef(fit)[2] * 0.34))
text(newx, rep(220, 3), labels = newx, pos = 4)
text(rep(0.12,3), round(newy, digits=2), labels =  round(newy, digits=2), pos = 3)
```

---

## Regresión usando ANOVA (1)

Usaremos la base de datos `mtcars` que viene en la librería `datasets`. Analizaremos cuál o cuáles variables nos permiten predecir la variable MPG (millas por galón) dado un conjunto de datos (rendimiento, peso, transmisión, etc) de varios modelos de automóviles.

Para ver las variables de esta base de datos hacemos lo siguiente:
```{r, echo=TRUE, eval=FALSE}
library(datasets) #This library provides free databases
data(mtcars) #The database I will use
str(mtcars) #str displays variables names and displays basic information
```
ANOVA explica las fuentes de variabilidad, es decir que puede ayudar a determinar cuáles son las variables que tienen efectos significativos estadísticamente hablando.

---

## Regresión usando ANOVA (2)

ANOVA permite determinar los efectos de todas las variables sobre MPG en el contexto de un modelo lineal. Usando `analysis <- aov(mpg ~ ., data = mtcars); summary(analysis)`
```{r, echo=FALSE}
analysis <- aov(mpg ~ ., data = mtcars) 
summary(analysis)
```

---

## Regresión usando ANOVA (3)

### Modelo 1

Estimaremos el siguiente modelo considerando los datos de la tabla ANOVA:
$$
MPG_i = \beta_0 + \beta_1 CYL_i + \beta_2 DISP_i + \beta_3 WT_i + \beta_4 AM_i + \varepsilon_i
$$
```{r}
fit1 <- lm(mpg ~ cyl + disp + wt + am, data = mtcars)
coefficients(fit1)
```

---

## Regresión usando ANOVA (4)

### Modelo 2

Estimaremos el siguiente modelo considerando la significancia de las variables:
$$
MPG_i = \beta_0 + \beta_1 CYL_i + \beta_2 WT_i + \beta_3 AM_i + \varepsilon_i
$$
```{r}
fit2 <- lm(mpg ~ cyl + wt + am, data = mtcars)
coefficients(fit2)
```

---

## Ejercicio

Escriba un informe de no más de 2 páginas junto a su grupo de trabajo (los mismos del proyecto de curso) en el que se responda claramente:
 
* ¿Cuáles son las variables más importantes para explicar la variable MPG en distintos modelos de automóviles?
* ¿Cuál de los dos modelos de la parte anterior es mejor y por qué?
* ¿Se puede decir que la variable AM (tipo de transmisión) explica la variable MPG en distintos modelos de automóviles? 
* Comente los alcances y limitaciones del modelo en base a los datos disponibles y los supuestos de OLS

Indicaciones:

* Use argumentos estadísticos y argumentos teóricos en base a prensa especializada
* Presente sus resultados en un lenguaje simple y formal
